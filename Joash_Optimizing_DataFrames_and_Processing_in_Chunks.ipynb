{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "y7eHGTSKv8TW",
        "ZJ7UJlKnwJk3",
        "StemrBe2wkMj",
        "8BtAVP8fw3OH",
        "22oYzgXnxIcV",
        "DMJVaj_kxj42"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7eHGTSKv8TW"
      },
      "source": [
        "# Project Notebook: Optimizing DataFrames and Processing in Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "project 2"
      ],
      "metadata": {
        "id": "zlKTovm7KiPr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ7UJlKnwJk3"
      },
      "source": [
        "## 1. Introduction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUyK3GcBv9mC"
      },
      "source": [
        "In this project, we'll practice working with chunked dataframes and optimizing a dataframe's memory usage. We'll be working with financial lending data from Lending Club, a marketplace for personal loans that matches borrowers with investors. You can read more about the marketplace on its website.\n",
        "\n",
        "The Lending Club's website lists approved loans. Qualified investors can view the borrower's credit score, the purpose of the loan, and other details in the loan applications. Once a lender is ready to back a loan, it selects the amount of money it wants to fund. When the loan amount the borrower requested is fully funded, the borrower receives the money, minus the origination fee that Lending Club charges.\n",
        "\n",
        "We'll be working with a dataset of loans approved from 2007-2011 (https://bit.ly/3H2XVgC). We've already removed the desc column for you to make our system run more quickly.\n",
        "\n",
        "If we read in the entire data set, it will consume about 67 megabytes of memory. Let's imagine that we only have 10 megabytes of memory available throughout this project, so you can practice the concepts you learned in the last two lessons.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Read in the first five lines from `loans_2007.csv` (https://bit.ly/3H2XVgC) and look for any data quality issues.\n",
        "\n",
        "2. Read in the first 1000 rows from the data set, and calculate the total memory usage for these rows. Increase or decrease the number of rows to converge on a memory usage under five megabytes (to stay on the conservative side)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc9NLSZ5vXPM"
      },
      "source": [
        "# Importing pandas\n",
        "import pandas as pd\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "# Your code goes here\n",
        "memory_footprints = []\n",
        "chunk_iter = pd.read_csv(\"https://bit.ly/3H2XVgC\", nrows = 5)\n",
        "print(chunk_iter)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading 1000 and calculating total  memory "
      ],
      "metadata": {
        "id": "yXYnN2wbIypN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = chunk_iter.dropna()"
      ],
      "metadata": {
        "id": "NnGDHpakJkCC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory_footprints = []\n",
        "chunk_iter = pd.read_csv(\"https://bit.ly/3H2XVgC\", nrows = 1000)\n",
        "print(chunk_iter)"
      ],
      "metadata": {
        "id": "VMfhA335I_nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_iter = pd.read_csv(\"https://bit.ly/3H2XVgC\")\n",
        "chunk_iter.info(memory_usage = \"deep\")"
      ],
      "metadata": {
        "id": "8-mtdC_GKJx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StemrBe2wkMj"
      },
      "source": [
        "## 2. Exploring the Data in Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm4p982Dwor5"
      },
      "source": [
        "Let's familiarize ourselves with the columns to see which ones we can optimize. In the first lesson, we explored column types by reading in the full dataframe. In this project, let's try to understand the column types better while using dataframe chunks.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "For each chunk:\n",
        "* How many columns have a numeric type? \n",
        "* How many have a string type?\n",
        "* How many unique values are there in each string column? How many of the string columns contain values that are less than 50% unique?\n",
        "* Which float columns have no missing values and could be candidates for conversion to the integer type?\n",
        "* Calculate the total memory usage across all of the chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many columns have a numeric type?"
      ],
      "metadata": {
        "id": "k9K2A9MxKcGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_iter = pd.read_csv(\"loans_2007.csv\", chunksize=1000, header=0)\n",
        "q = ['float64']\n",
        "\n",
        "for chunk in chunk_iter:\n",
        "  chunk_iter_q = chunk.select_dtypes(include=q)\n",
        "  print(len(chunk_iter_q.columns))\n"
      ],
      "metadata": {
        "id": "rT4GVrtBJaAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many have a string type?"
      ],
      "metadata": {
        "id": "o_kLij_a0Jhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_iter = pd.read_csv(\"loans_2007.csv\", chunksize=1000, header=0)\n",
        "for chunk in chunk_iter:\n",
        "  str_type = chunk.select_dtypes(include=['object']).columns\n",
        "  print(len(str_type))"
      ],
      "metadata": {
        "id": "8Lv-gCNm0LxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the total memory usage across all of the chunks."
      ],
      "metadata": {
        "id": "w1MG2F7Z3Lx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_iter = pd.read_csv(\"loans_2007.csv\", chunksize=1000, header=0)\n",
        "for N in chunk_iter:\n",
        "  print(N.info(memory_usage=\"deep\"))"
      ],
      "metadata": {
        "id": "er5TKieh3NfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many unique values are there in each string column? How many of the string columns contain values that are less than 50% unique?"
      ],
      "metadata": {
        "id": "LKpxUSy40nas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_iter = pd.read_csv(\"loans_2007.csv\", chunksize=1000, header=0)\n",
        "for val in chunk_iter:\n",
        "  for col in val.select_dtypes(include=['object']):\n",
        "    print(f'{col}',len(val[col].unique()))"
      ],
      "metadata": {
        "id": "PqwsGu8x0o-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BtAVP8fw3OH"
      },
      "source": [
        "## 3. Optimizing String Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkWArHAhw_bw"
      },
      "source": [
        "We can achieve the greatest memory improvements by converting the string columns to a numeric type. Let's convert all of the columns where the values are less than 50% unique to the category type, and the columns that contain numeric values to the `float` type.\n",
        "\n",
        "While working with dataframe chunks:\n",
        "* Determine which string columns you can convert to a numeric type if you clean them. For example, the `int_rate` column is only a string because of the % sign at the end.\n",
        "* Determine which columns have a few unique values and convert them to the category type. For example, you may want to convert the grade and `sub_grade` columns.\n",
        "Based on your conclusions, perform the necessary type changes across all chunks. * Calculate the total memory footprint, and compare it with the previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH0tcQlpxG9s"
      },
      "source": [
        "# Your code goes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine which string columns you can convert to a numeric type if you clean them. For example, the int_rate column is only a string because of the % sign at the end."
      ],
      "metadata": {
        "id": "i4yfvmkZ3rFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_iter = pd.read_csv(\"loans_2007.csv\", chunksize=1000, header=0)\n",
        "for NUMERIC in chunk_iter:\n",
        "\n",
        "  NUMERIC['int_rate'] = NUMERIC['int_rate'].str.replace('%', '')\n",
        " \n",
        "\n",
        "  cols = ['int_rate']\n",
        "  for col in cols:\n",
        "    NUMERIC[col] = NUMERIC[col].astype('float')\n",
        "print(NUMERIC)"
      ],
      "metadata": {
        "id": "EYy-GOdh3-c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine which columns have a few unique values and convert them to the category type. For example, you may want to convert the grade and sub_grade columns. Based on your conclusions, perform the necessary type changes across all chunks. * Calculate the total memory footprint, and compare it with the previous one."
      ],
      "metadata": {
        "id": "k1esSSHG5bR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_iter  = pd.read_csv(\"loans_2007.csv\", chunksize=1000, header=0)\n",
        "for U in chunk_iter :\n",
        "  for col in U.select_dtypes(include=['object']):\n",
        "    sum = len(U[col])\n",
        "    U_sum = len(U[col].unique())\n",
        "    Q = (U_sum/sum)*100\n",
        "    if Q < 50:\n",
        "      U[col] = U[col].astype('category')\n",
        "print(U)"
      ],
      "metadata": {
        "id": "ZgweM7pe5dgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22oYzgXnxIcV"
      },
      "source": [
        "## 4. Optimizing Numeric Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5C20YrxPID"
      },
      "source": [
        "It looks like we were able to realize some powerful memory savings by converting to the category type and converting string columns to numeric ones.\n",
        "\n",
        "Now let's optimize the numeric columns using the `pandas.to_numeric()` function.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "While working with dataframe chunks:\n",
        "* Identify float columns that contain missing values, and that we can convert to a more space efficient subtype.\n",
        "* Identify float columns that don't contain any missing values, and that we can convert to the integer type because they represent whole numbers.\n",
        "* Based on your conclusions, perform the necessary type changes across all chunks.\n",
        "* Calculate the total memory footprint and compare it with the previous one.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "chunk_iter = pd.read_csv(\"loans_2007.csv\", chunksize=1000, header=0)\n",
        "for q in chunk_iter:\n",
        "  q_columns = q.select_dtypes(include=['float64'])\n",
        "  num = q_columns.columns[q_columns.isna().any()].tolist()\n",
        "  q[num] = q[num].fillna(0)\n",
        "  storage = (q.memory_usage(deep=True).sum())\n",
        "  print(q.info(memory_usage=\"deep\"))"
      ],
      "metadata": {
        "id": "0ggZZGt0uQlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMJVaj_kxj42"
      },
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0RL3BzexlkW"
      },
      "source": [
        "We've practiced optimizing a dataframe's memory footprint and working with dataframe chunks. Here's an idea for some next steps:\n",
        "\n",
        "Create a function that automates as much of the work you just did as possible, so that you could use it on other Lending Club data sets. This function should:\n",
        "\n",
        "* Determine the optimal chunk size based on the memory constraints you provide.\n",
        "\n",
        "* Determine which string columns can be converted to numeric ones by removing the `%` character.\n",
        "\n",
        "* Determine which numeric columns can be converted to more space efficient representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hvuUNzPx1zy"
      },
      "source": [
        "# Your code goes here\n",
        "import pandas as pd\n",
        "\n",
        "chunk_iter = pd.read_csv(\"loans_2007.csv\", chunksize=1000, header=0)\n"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}